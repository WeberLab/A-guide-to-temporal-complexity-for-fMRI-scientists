In the 21st century, complexity science has received increasing attention (Varley et al., 2020). Rooted in mathematics (Burggren & Monticino, 2005), complexity science is an interdisciplinary toolbox of frameworks, methods, and approaches that allow for the description of systems with multiple components that interact across scales, including of emergent and unpredictable behaviours. The great promise of complexity science is that it could capture thus-far ineffable aspects of real-world systems, enabling progress where linear methods have stalled (Jayasinghe et al., 2012; Hernández et al., 2023; Yang & Tsai, 2013; e.g., Safavi et al., 2024; Mormann et al., 2007; Casali et al., 2013; Mizuno et al., 2010). 

The brain is an ideal candidate for complex-systems approaches, as it consists of components that interact across scales (Castiglioni et al., 2020; Fig. 1). Neural complexity has been explored from very small scales to very large—including at the cellular level using spikes, intracellular recordings, and local field potentials, and at the whole-brain level using electroencephalography (EEG), magnetoencephalography (MEG), functional near-infrared spectroscopy (fNIRS), electrocorticography (ECoG), and functional magnetic resonance imaging (fMRI) (Cofre & Destexhe, 2025). As for complexity metrics, there exists a vast array (Fig. 2). As reviewed by Sarasso et al. (2021), these include methods from graph theory (e.g., network modularity, density, node centrality, clustering coefficient, local and global efficiency, and small-worldness), the quantification of integration and differentiation (e.g., integrated information), the entropy of functional connectivity (FC), measures of information sharing (e.g., mutual information and transfer entropy), indices of the distribution of microstates, and perturbational complexity (for more reviews, see Sun et al. (2020), Hernández et al. (2023), Keshmiri (2020), and Yang & Tsai (2013)). These metrics reflect varying motivations and theoretical constructions, and there is no unified framework for their applications (Cofre & Destexhe, 2025). 

Neuroimaging complexity measures can be classified as spatial, temporal, or spatiotemporal, reflecting the multidimensionality of neuroimaging data. Spatial measures describe brain complexity across space at one moment (for instance, the complexity of cortical structures; Kiselev et al., 2003). Temporal measures, also called moment-to-moment measures, describe time series from individual regions (e.g., voxels, functional areas, parcels, or networks), excluding spatial information. Spatiotemporal measures incorporate both spatial and temporal information, yielding a four-dimensional characterization of brain function (e.g., measures based on dynamic FC). Because spatiotemporal measures incorporate both dimensions, they are assumed to most accurately reflect actual brain complexity (Hull & Morton, 2023). Despite this, most neural complexity studies use temporal measures (Hull & Morton, 2023; see Sun et al., 2020 for Alzheimer’s). 

Within modalities with high temporal resolution—EEG and MEG—temporal complexity is standard usage, with mature terminology and pipelines (e.g., Neurokit2; Makowski et al., 2021). Despite the fact that fMRI is increasingly widely used for neuroscience and clinical research (Baillet, 2017), the fMRI-temporal-complexity literature is diffuse and lacks standardization across siloed research groups. Researchers may assume fMRI is unsuitable for temporal complexity methods for a variety of reasons: due to its low sampling rate (typically between a little over half a second to three seconds) and slow dynamics (Figs. 3, 4); due to the fact that fMRI studies usually involve short runs, which makes it challenging to apply metrics that require a large number of data points (Grandy et al., 2016); and due to limits to the interpretability of the BOLD response, which is a lagging, distorted proxy of neuronal activity (Tsvetanov et al., 2015; Epp et al., 2025; Wiafe et al., 2025). 

Despite its limitations and unorthodoxy, fMRI temporal complexity has been repeatedly shown to discriminate between different cognitive, physiological, and disease states (e.g., Sun et al., 2020; Hernández et al., 2023). Additionally, it has several advantages over EEG and MEG: Due to its high spatial resolution, fMRI can be used to describe how complexity varies across regions (e.g., Nezafati et al., 2020) and how it relates to high-spatial-resolution nonlinear measures (e.g., FC) (Grieder et al., 2018). FMRI is also distinguished in its ability to measure information content in lower frequencies, such as those comprising resting-state networks (approximately .01 to .1 Hz), that can only be suboptimally obtained via downsampling with EEG/MEG (Cohen, 2014). Another advantage of fMRI is that it can be combined with other modalities, such as spectroscopy (to describe metabolism), diffusion-weighted imaging (to describe structural connectivity), or EEG (Krakow et al., 2005). Complexity more broadly has been used in fMRI: A search of large-scale non-invasive neuroimaging studies examining complexity in neurodegenerative disease found that around 7% involved fMRI or fNIRS; the remainder used EEG or MEG (Sun et al., 2020), a search of studies on consciousness and criticality found that approximately 25% of human studies used fMRI (Walter & Hinterberger, 2022), and a search of studies on criticality found that approximately 30% used BOLD (Hengen & Shew, 2024). 
Images (click arrow to open drop-down)

Figure 1. The multiscale brain. The brain is complex at the microscopic, mesoscopic, and macroscopic scales. Cartoons adapted from Ros et al. (2014); BOLD time series adapted from Cifre et al. (2020); layout inspired by Betzel & Bassett (2016).




Figure 2. A variety of approaches to nonlinear analysis of neuroimaging data. Word cloud was created using wordcloud.com, with keywords taken from reviews of nonlinearity/complexity including Sarasso et al. (2021), Sun et al. (2020), Hernández et al. (2023), Keshmiri (2020), Donoghue et al. (2024), and Yang & Tsai (2013). Word sizes are not necessarily related to frequency.


Figure 3. Among neuroimaging methods, fMRI has a low temporal resolution. Here, one second of BOLD signal (TR = 1s in this example) is contrasted with one second of MEG signal (TR < 0.001s). Each timepoint in the BOLD signal represents activity for the entire two-second window. Arbitrary units; note that BOLD and MEG signals represent different physiological processes. Original image. 




Figure 4. Stylization of the spatial and temporal resolutions of noninvasive neuroimaging methods. Image based on van Gerven et al. (2009). We note that these modalities are placed on the same axes for pedagogical purposes; more strictly, the signal across modalities represent differing neurophysiological processes that lack straightforward comparability (e.g., Itthipuripat et al., 2019).


Figure 5. This review focuses on fMRI time series data, ignoring the functional relationships between adjacent voxels. 


Side bar (click arrow to open drop-down)
Text Box 1

What’s the difference between temporal complexity and variability?

Temporal signal variability quantifies time series irregularity (Fig. 6). Neuroimaging analyses have traditionally aimed to describe central tendency, ignoring or removing variability. However, far from being irrelevant noise, variability is essential to brain function and of interest in its own right (Garrett et al., 2013; Baracchini et al., 2023). Measures of temporal signal variability have been used to investigate cognition, lifespan development, and disease (Steinberg & King, 2024; Goodman et al., 2024; Chen et al., 2022; Wei et al., 2023). These measures include variance (He, 2011), standard deviation (SD; the square root of the variance; Garrett et al., 2010), and mean square successive differences (similar to variance, but comparisons are made between adjacent timepoints in the series; Leo et al., 2012; Afyouni & Nichols, 2018).

Any discussion of complexity would be incomplete without mention of variability. In fact, the two terms are often used interchangeably (e.g., Jaworska et al., 2018) despite their statistical distinction. Variability describes the spread of a signal, while complexity describes within-series interactions and recurring patterns. A time series that is more complex is not necessarily more variable, and a more variable time series is not necessarily more complex. Indeed, some measures of complexity explicitly scale out variability (e.g., ApEn; Richman & Moorman, 2000). 

Still, variability and complexity are closely intertwined. A system with no variability (i.e., is completely ordered) would not be complex, and neither would a system that exhibits very high moment-to-moment variability (i.e., is completely random). That is, complexity relies on a particular  degree of variability (e.g., Lofti et al., 2021; Garrett et al., 2013; Chialvo, 2018; He, 2011; Steinberg & King, 2024; Goodman et al., 2024). 

A review of empirical comparisons between variability and complexity in fMRI is provided in the later section “Comparisons of All Metrics.” Briefly, empirical results confirm that the two constructs are complementary. 



Figure 6. Conceptual comparison of the features captured by the mean (a measure of central tendency), variance (a measure of variability), and Hurst exponent (a measure of complexity). Synthetic waveform from Li et al. (2012); figure inspired by Garrett et al. (2010). 


The scope of this review

The current review focuses on temporal measures of complexity in fMRI. We focus solely on voxel-by-voxel measures of temporal complexity, excluding measures that incorporate spatial information. That is, we focus on time series within single voxels, excluding any measure that considers the relationships between voxels. For instance, we exclude measures that quantify the complexity of FC networks (Fig. 5). We also exclude methods that apply traditionally voxel-by-voxel temporal complexity measures to concatenated multiple voxel time series (e.g., concatenated Lempel-Ziv complexity (LZC)), and we exclude studies using simulated fMRI data. 

Previous reviews of complexity in fMRI exist (including Sokunbi et al., 2016; Xin et al., 2021), but our review differs in thoroughness: We attempted to identify all the temporal complexity metrics used in fMRI and all the papers for each metric. To our knowledge, given the vast scope of this endeavour, no previous review has done this. 

This review additionally differs from previous reviews in its aim: Rather than to summarize previous clinical and neuroscience findings (although we do this), our primary aim is to provide guidance to clinical/cognitive researchers for implementing each metric. That is, our review addresses a problem: Many clinical/cognitive fMRI researchers seeking to apply complexity measures do not have a background in information theory and thus face a choice between applying temporal complexity measures without a clear understanding of the context (potentially resulting in mistakes in methodology or interpretation) or learning about this context, which, given that fMRI complexity is a vast interdisciplinary topic and explanations are scattered across disciplines with varying levels of technical detail, involves significant time commitment. Our review aims to solve this problem by being a comprehensive resource for temporal complexity methods in fMRI. 
1. Shannon entropy

Shannon entropy is central to all the entropy measures we will discuss in this review. The development of this measure marked the beginning of the field of information theory (Shannon, 1948). Shannon entropy quantifies uncertainty; it can be thought of as the difficulty of predicting an observation’s value (Figs. 6, 7). Applied to a time series, it measures the minimum number of bits needed to encode the series based on the frequency of values.  

where H is entropy and pi is the frequency of a given value. 

Shannon entropy is a widespread measure of spatiotemporal complexity (e.g., the complexity of FC networks; Viol et al., 2017; Pappas et al., 2019). However, in its simplest formulation, Shannon entropy is rarely used to describe the temporal complexity of neuroimaging time series. This is because the formula considers only value frequency, not order, and so the resulting metric is poorly suited to describing how a series changes over time. Only one study has used Shannon entropy of frequency of values: Huang et al., 2025 (details in discussion). Beyond time-domain applications, several studies have applied Shannon entropy to wavelet-decomposed time series (i.e., analyzing the entropy of different frequency bins; method described in Rosso et al., 2001), which is an improvement to classic Shannon entropy (e.g., Gupta et al. (2017) found that Shannon wavelet entropy was able to discriminate resting-state data from epilepsy patients versus controls, whereas Shannon entropy was not). 

Although Shannon entropy is rarely applied with the explicit aim of describing temporal complexity, it has been used as an innovative alternative to traditional methods of analyzing event-related fMRI data. Unlike traditional methods, entropy makes no assumptions about the shape of the evoked hemodynamic response, which can improve stability and sensitivity to activation (Mikolas et al., 2012; Ostwald & Bagshaw, 2011). De Araujo et al. (2003) were the first to apply Shannon entropy to event-related fMRI analysis. They calculated entropy within time windows during activation and rest; the rise and fall of the entropy measure mirrored the theoretical hemodynamic response (de Araujo et al., 2003; Fig. 8). Compared to traditional cross-correlation, Shannon entropy was more stable with a changing signal-to-noise ratio (de Araujo et al., 2003). DiNuzzo et al. (2017) also used Shannon entropy in event-related analysis, which allowed them to capture changes induced by photic stimulation that were missed by traditional temporal descriptors (e.g., variance). Together, these results indicate that temporal entropy may capture features of BOLD dynamics that escape conventional temporal measures (DiNuzzo et al., 2017). More broadly, this demonstrates the usefulness of temporal dynamics in situations where spatial maps are unable to fully capture condition differences (DiNuzzo et al., 2017).

Several extensions to Shannon entropy have been introduced as alternatives to conventional methods for discriminating task conditions in event-related fMRI. A comprehensive list of these methods and their applications is as follows: Tsallis entropy (applied by Sturzbecher et al., 2009; Wang et al., 2013), Renyi entropy (applied by Gonzalez Andino et al., 2000; Wang et al., 2013), generalized relative entropy (applied by Cabella et al., 2009; Welvaert & Rosseel, 2012), adaptive entropy rate (applied by Fisher et al., 2001), and time-lagged mutual information (applied by Tsai et al., 1999; Fuhrmann Alpert et al., 2007, 2008; Tedeschi et al., 2004, 2005; von Wegner et al., 2018). Despite their potential effectiveness in event-related fMRI analysis, these methods have not been widely adopted, neither since they were first reviewed by Mikolas et al. (2012) nor in the decade since. This may be because of their high computational demands (Mikolas et al., 2012) or simply because there are better approaches to model-free hemodynamic response estimation (e.g., finite-impulse-response models (Metzak et al., 2011)). No studies have systematically evaluated whether Shannon-entropy-based analysis methods discriminate task conditions better than conventional ones, which leaves a knowledge gap. 
Images (click arrow to open drop-down)


Figure 6. Shannon entropy represents uncertainty about an observation's value. Image adapted from Serrano (2017).  





Figure 7. Shannon entropy reflects the distribution of values; a wider distribution of BOLD signal values (blue) has higher entropy, while a narrower distribution of values (green) has lower entropy. Shannon entropy of FC follows the same pattern: For instance, for entropy of path lengths between nodes (Viol et al., 2019), a broader distribution of path lengths corresponds to higher FC entropy, and a narrower distribution corresponds to lower FC entropy. Original image. 


Figure 8. An example of how Shannon entropy can be used to analyze event-related data (de Araujo et al., 2003). Image stylized/recreated from de Araujo et al. (2003).


2. Kolmogorov-Sinai entropy
In contrast to Shannon entropy, Kolmogorov-Sinai (KS) entropy takes time into account (Fig. 9), allowing for application to dynamical systems (Sinai, 1959; Nogueira, 2017). As a system evolves, KS entropy quantifies the average rate at which information is produced with each new state; that is, it quantifies the difficulty of predicting future observations given past observations. Higher KS entropy implies that a higher amount of information is being introduced at each time point, meaning that the series is more unpredictable. Like Shannon entropy, KS entropy implements the intuitive notion that a broader distribution of data values should correspond to greater uncertainty. 

KS entropy is difficult to apply to real-world data, given that proper implementation requires exceedingly large amounts of time series data and an absence of noise processes (Pincus, 1991; Richman & Moorman, 2000). As such, a variety of measures have been developed as approximations for KS entropy—distinguished from KS by the fact that they are possible to compute. The next sections discuss approximations of KS entropy that have been applied to fMRI research.
Images (click arrow to open drop-down)

Figure 9. Because Shannon entropy only depends on the distribution of values and not their order, two sequences may look different but have the same Shannon entropy. Here, both series have the same Shannon entropy, but the series on the right has higher KS entropy. Original image. 


3. Approximate entropy
In fMRI, nearly all approximations of KS entropy can trace their historical lineage to approximate entropy (ApEn). ApEn was one of the first approximations of KS entropy developed for physiological data (Pincus, 1991). First used on cardiovascular time series and later adapted for neuroimaging (Richman & Moorman, 2000; Bruhn et al., 2000), ApEn quantifies the probability that sequences of similar patterns in a time series will remain similar when sequence length is increased. A higher value of ApEn signifies that the signal contains fewer repeating patterns—that is, greater complexity. For a more detailed explanation, see the “Sample Entropy” section.

In the calculation of ApEn, each sequence is counted as matching itself (Richman & Moorman, 2000). This nuance leads to two limitations. Firstly, it causes ApEn to be lower than expected for shorter time series (Richman & Moorman, 2000). This is because counting self-matches inflates the estimation of regularity, and this over-inflation is more prominent for short series. Shortening datasets (e.g., by using one fMRI run instead of two) may change ApEn despite identical patterns. Secondly, ApEn lacks relative consistency, meaning that comparisons between datasets can be affected by the choice of time-window and tolerance parameters (i.e., if ApEn is higher for one dataset than another using one set of parameters, we would expect this to remain true for other choices of parameters, but this is not the case; Richman & Moorman, 2000). 

Given these two important limitations, ApEn is rarely used to describe temporal complexity in fMRI. In the fMRI literature, most applications of ApEn appear alongside other temporal entropy measures, such as SE and fuzzy approximate entropy; thus, these applications are listed in the “Comparisons of All Metrics” section of this paper. Two ApEn studies that are not listed in this section are: Sokunbi et al. (2011) and Liu et al. (2012); both these studies’ clinical findings were consistent with the broader complexity literature and are described in the discussion.  
4. Sample entropy
Sample entropy (SE) was developed to correct ApEn’s limitations (Richman & Moorman, 2000). The difference between the calculation of SE and that of ApEn is that SE doesn’t count self-matches in the conditional probability, which corrects the two limitations described in the ApEn section (Richman & Moorman, 2000; example in fMRI: Yang et al., 2018). In addition to these advantages, SE is a better approximation of KS entropy and is simpler to compute (Richman & Moorman, 2000). Accordingly, SE is more widely used than ApEn in fMRI and can be computed using the toolboxes BENtbx (https://github.com/zewangnew/BENtbx; e.g., Wang et al., 2014) or Complexity Toolbox (http://loft-lab.org/index-5.html; e.g., Zhang et al., 2021). SE is most often called “brain entropy” (BEN) (Wang et al., 2014); however, because BEN can also refer to Shannon entropy (Akdeniz, 2017), we elected to use the term SE in this paper. 

To calculate SE, the user specifies two parameters: the time window m and the tolerance scale r. m is the number of values to be analysed at a time, and r is the multiplied by the SD of the series to obtain the tolerance. SE is equal to the negative average natural logarithm of the conditional probability that two sequences that are similar (i.e., within the tolerance) for m points will stay similar for m+1 points (Richman & Moorman, 2000). Because conditional probability is between 0 and 1, SE will always be positive. When the time series is highly regular (i.e., when similar runs remain similar), SE is low; when the time series is irregular, SE is high (Pincus, 1991). See Figure 10 for a conceptual explanation, Figure 11 for a computational explanation, and Delgado-Bonal & Marshak (2019) for a tutorial. For a detailed guide to selecting m and r, see Appendix A1 and Figure A1. 

What is the minimum number of timepoints needed for SE? In general, more timepoints improve results. For example, using simulated data, Grandy et al. (2016) found that SE accuracy and precision increased fairly linearly from the shortest to longest series tested (32 to 32,768). Wehreim et al. (2024) (Fig. 12) observed strong (albeit non-significant) correlations between reliability and series length, including a slow increase in split-half correlation from the shortest to longest series tested (100-800), along with a steady increase in test-retest correlation that plateaued at 500. Highlighting that the relationship between length and reliability is dataset-dependent, they found differences in rest versus task data (for a wide variety of tasks, including cognitive, motor, and social). As for the lower limit of length, Yang et al. (2018) identified a lower bound of 97 timepoints; for series shorter than 97, SE could still be calculated, but with a narrower range of m- and r-values. Sokunbi (2014) found that SE discriminated between young and old adults in series as short as 85 in a small (N = 20) cohort. Again highlighting that length limitations are dataset-dependent, Sokunbi found that more data points were needed in a larger (N = 86) cohort (potentially because the smaller cohort had low inter-individual variability). 

Several innovations to SE have been made in recent years. Del Mauro & Wang (2024) introduced “cross” SE, a method for comparing multiple SE maps which can be applied to make comparisons between subjects, sessions, scan times, or regions. Thus far, cross entropy has been used to show a decoupling between the (across-subject) mean and variance of SE across different brain regions (Del Mauro & Wang, 2024). Wang (2021) pioneered dynamic SE, wherein SE is estimated for every segment of a sliding window. Despite its name, dynamic SE is not designed to be used to track changes in SE over the course of a run; instead, SE for each segment is combined into a final run mean. In a sample of 862 subjects, Wang found minimal differences between static (traditional) and dynamic SE. 

Table 1 includes a complete list of fMRI-SE studies; Figure 13 depicts the trend in the yearly number of studies published. 

The entropy-based metrics discussed thus far—Shannon entropy, KS entropy, ApEn, and SE—are maximized with maximum randomness (Fig. 14). That is, these metrics are formulated to increase monotonically with series randomness. Contrast this with the vision articulated by Sporns (2016), wherein complexity is conceptualized as involving a balance between order and disorder. Uniform systems (e.g., crystal lattices; sine waves) are not complex, nor are completely irregular ones (e.g., randomly moving particles; white noise); rather, complexity lies between these two extremes, combining predictability and unpredictability (Varley et al., 2020). Hence, an “ideal” complexity metric should be maximized at an intermediate point between irregularity and regularity. Therefore, strictly put, despite being the most commonly used metrics, ApEn and SE do not truly capture “complexity.” 

How much of a limitation is this? We are reluctant to dismiss the breadth of good work that has already been completed; in our view, the conflation of complexity and irregularity is only a limitation if, at the scale under investigation, the comparison involves shifts past the midpoint from the domain of irregularity to that of order. Although we recognize the limitations of comparing across metrics, we observe that metrics that can describe the full irregularity-regularity range tend to regularity in the adult brain (e.g., avalanche: Xu et al., 2021; Hurst: Campbell et al., 2022; described further later in this review). Therefore, the distinction between complexity and irregularity may be only semantic, at least in the healthy adult brain (but perhaps not in other populations—e.g., infants: Mella et al., 2024). Such distinctions could account for the observed inconsistencies across clinical studies, especially across neuroimaging modalities. In the following section, we will discuss a method to address this limitation: MSE. 
Images (click arrow to open drop-down)


Figure 10. Conceptual explanation of SE. SE represents the probability that similar sequences will stay similar. Original image. 





Figure 11. An example SE calculation with m = 2. Original image, except for time series (black dots/lines) which is adapted from Roediger et al. (2024). 





Figure 12. SE accuracy (and precision, not shown) increase with increasing series length. Figure stylized based on results from Wehreim et al. (2024).


Figure 13. Trend in the number of studies published using SE or MSE in fMRI. No fMRI-SE/MSE studies were found prior to 2013. Year 2025 was excluded.


Figure 14. Entropy measures (Shannon entropy, sample entropy, approximate entropy, and others; but not multiscale sample entropy) increase monotonically with data randomness (A). In contrast, an ideal complexity measure would represent the balance between order and disorder (B). Image adapted from Yang et al. (2013).


5. Multiscale sample entropy
The metrics discussed thus far—Shannon entropy, ApEn, and SE—are considered single-scale metrics, as they consider patterns within a single time-window size. Is a single scale sufficient to describe the brain? The brain is complex across a range of temporal scales, from very short time windows to very long ones, and understanding how these layers interact is essential to understanding the system as a whole (Beggs, 2022; Buzsáki, 2009; Betzel & Bassett, 2017). For example, in cross-frequency coupling, high-frequency oscillations (i.e., patterns that recur at fine scales) interact with slow ones, allowing for information transmission (Canolty & Knight, 2010). More broadly, fractality, which will be discussed later in the paper, is essential to brain function (Werner, 2010). A better complexity metric would therefore have the ability to describe patterns that occur across multiple scales. An additional limitation of ApEn and SE is that these measures increase monotonically as the irregularity of the system increases, meaning that they do not capture the conceptualization of complexity as involving  a balance between order and disorder (as described in the SE section). Thus, single-scale measures of entropy fail to capture the full extent of the information contained within the BOLD signal. 

MSE was developed to more fully characterize the complexity of physiological signals by describing SE over multiple scales (Costa & Healey, 2003; Yang et al., 2013). This is achieved by downsampling the original time series to multiple lower temporal resolutions to create “new” series across a range of lower frequencies (Fig. 15). The SE algorithm is then applied to each series, resulting in a unique SE value for each temporal resolution. That is, the final output consists of a vector of SE values, one for each resolution. Because the output isn’t a single number, it can be hard to present and interpret. Typically, results are presented as a plot of SE versus sampling resolution (e.g., Fig. 16). As an attempt to summarize the output, the slope of this plot or the mean value across scales may also be reported. 

Unlike single-scale entropy, MSE can differentiate randomness from complexity. The shape of the scale-MSE curve is neurophysiologically meaningful (Fig. 16). MSE for white noise is high at short scales (where there are random fluctuations) and decreases at coarser scales, as fluctuations are smoothed out (McDonough et al., 2014; Ho et al., 2017; but this may be due to bias, see Kosciessa et al., 2020). Fully preprocessed cerebrospinal fluid signal, which (in theory) consists of a series of uncorrelated random observations and therefore approximates white noise, likewise has a high-then-low MSE curve (McDonough et al., 2014). On the other hand, complex signals, which contain meaningful information across scales, have approximately horizontal MSE curves. For instance, pink and brown noise—which, unlike white noise, contain autocorrelation (i.e., future values are influenced by past ones)—both have characteristic MSE curves (Fig. 16), with pink noise, which contains more autocorrelation than brown, having the flatter curve. (McDonough et al., 2014; Omidvarnia et al., 2021; Ho et al., 2017; Smith et al., 2014). Consistent with the idea that BOLD signal from grey matter contains meaningful information, its MSE more closely resembles that of pink noise than that of white noise (McDonough et al., 2014; Omidvarnia et al., 2021; Ho et al., 2017; Smith et al., 2014). See Appendix A2 for an in-depth discussion of the significance of the MSE curve in grey matter, and for a discussion of the major limitations of MSE. See Appendix A3 for detailed instructions on choosing parameters for MSE calculation, including m and r. 

What are the limitations of MSE? MSE requires a long time series for proper computation and utilization. Series must be long enough such that the longest scale (e.g., lowest temporal resolution) produced through downsampling contains at least 80 timepoints (Sokunbi, 2014). Furthermore, for MSE to be most useful, the series must be long enough to support multiple resolutions of downsampling. Fortunately for fMRI researchers, Grandy et al. (2016) showed that MSE can be effectively estimated across discontinuous segments (i.e., multiple runs of fMRI can be concatenated and the estimation will still be effective). 

Table 1 includes a complete list of fMRI-MSE studies. 
Images (click arrow to open drop-down)


Figure 15. An illustration of downsampling of a time series. Image recreated from Kadota et al. (2021). 



Figure 16. MSE profiles for simulated white, pink, and red noise; preprocessed CSF; and grey matter (resting-state networks). Figure stylized based on results from McDonough et al. (2014). Note that scale is not the same as frequency, as scale and frequency are terms from entirely different frameworks; however, were these concepts to be compared, fine scales (low m values) would roughly map to high frequencies (fast-moving oscillations) and coarse scales (high m) to low frequencies (slow drifts). 


6. Other entropy-related measures

Beyond Shannon entropy, ApEn, SE, and MSE, a range of other entropy metrics are available (Fig. 17). These metrics are commonly used in other neuroimaging modalities (e.g., Keshmiri, 2020), but are seldom employed in fMRI. Table 3 lists fMRI studies using these metrics. 

Permutation entropy

Permutation entropy (PE) considers only the order of amplitude values, not absolute amplitudes (Bandt & Pompe, 2002; Figs. 18). PE calculation uses a sliding window to slice the series into overlapping segments called “embedded vectors.” Each embedded vector is matched to a motif (called a “permutation pattern” or an “ordinal pattern”), which represents the relative order of the values in the vector. PE is the Shannon entropy of the relative frequencies of the ordinal patterns. That is, PE is closely related to Shannon entropy, but considers the order of values. Compared to ApEn and SE, PE is simpler to compute, makes fewer assumptions, and is more robust in the presence of noise (Zanin et al., 2012). 

There are several variants of PE. PE can be computed on downsampled versions of the data (i.e., multiscale permutation entropy), resulting in a description of PE across frequencies (Wohlschläger et al., 2018). Weighted permutation entropy (wPE) was introduced by Fadlallah et al. (2013). wPE incorporates amplitude information into the PE calculation by multiplying each embedded vector by a weight (Fadlallah et al., 2013). Unlike PE, wPE is affected by spikes and abrupt amplitude changes (Fadlallah et al., 2013). 

Fuzzy entropy
Fuzzy entropy (FuzzyEn) is identical to SE but defines similarity differently (Chen et al., 2019). SE defines similarity using the Heaviside function; unfortunately, this function has a rigid boundary, leading to limitations including information loss and parameter-dependence. In contrast, FuzzyEn defines similarity using a fuzzy function and is thus an improved measure of complexity. 

Permutation fuzzy entropy
Permutation fuzzy approximate entropy (PFE) was introduced by Niu et al. (2020). It is calculated by first performing permutations on the original time series—which reduces the impact of noise—then computing FuzzyEn. Note that, despite its name, PFE is not directly related to PE.

Range entropy
Range entropy (RangeEn) was introduced to address a limitation of SE and ApEn that is pertinent in EEG, which is that these metrics are not robust to variations in signal amplitude (Omidvarnia et al., 2018). Compared to SE and ApEn, RangeEn is also less affected by variations in signal length, which makes it an option for short-length fMRI time series (Omidvarnia et al., 2018; Omidvarnia et al., 2023). There are two versions of RangeEn: RangeEnA is an improvement to ApEn, and RangeEnB is an improvement to SE (Omidvarnia et al., 2018). 

Dispersion entropy
Dispersion entropy (DispEn) originates from SE and PE and corrects a few problems with these respective techniques—namely, that SE is slow to calculate, and that PE does not thoroughly describe changes in amplitude (Rostaghi & Azami, 2016). DispEn is faster to compute than both SE and PE and a better descriptor of frequency and amplitude changes than PE (Rostaghi & Azami, 2016). 

Differential entropy
Differential entropy (DiffEn) describes the spread of the probability density function for a random variable (Cover & Thomas, 2005). It has an interesting history; Claude Shannon thought it was the analogue of discrete entropy for continuous variables, but it is not, and thus is not derived from information-theoretic first principles (Cover & Thomas, 2005). Hence, it changes with simple operations to the series like scaling or shifting, and its values are not always meaningful (as they are sometimes negative) (Cover & Thomas, 2005). Despite these major limitations, DiffEn has been used in EEG (e.g., Duan et al., 2013); in fMRI, it has been shown to have test-retest reliability comparable to or exceeding those of other temporal complexity metrics (Wang et al., 2013; Guan et al., 2022), though no other fMRI studies have used it. 
Images (click arrow to open drop-down)

Figure 17. Historical development of several entropy-related measures used in fMRI time series. Time proceeds down arrowheads. This is not comprehensive; for instance, RangeEn can be computed using the algorithm for ApEn as well as SE, but only SE is displayed for simplicity. Temporal complexity measures are much more extensively used in EEG/MEG than in fMRI; accordingly, the entropy measures identified in our search represent a small subspace of what is possible. Also, note that nearly all these measures have multiscale versions—e.g., multiscale sample entropy (MSE), multiscale permutation entropy, and multiscale fuzzy entropy—not displayed. 



Figure 18. Calculation of PE with embedded dimension (i.e., sliding window length) m = 1 and a scale factor (i.e., downsampling resolution) of 1. There are m! possible motifs (panel a). The signal is deconstructed into embedded vectors (panel b), which are matched to motifs. PE is the Shannon entropy of the relative frequency of the motifs (panel c). Image from Tosun et al. (2019).


7. Lempel-Ziv complexity

Lempel-Ziv complexity (LZC; Zenil, 2020) estimates Kolmogorov complexity. Kolmogorov complexity originates from computer science and is a measure of the computational resources needed to specify an object—for instance, the length of the shortest computer program required to generate a patterned series. The Lempel-Ziv algorithm parses the time series, identifying the number of unique patterns that make up the signal. Sequences with a small number of unique patterns (low LZC) are considered less complex, while sequences with a large number of unique patterns (high LZC) are considered more complex (Lempel & Ziv, 1976; Fig. 19). 

Over the past two decades, LZC has been the most common measure of complexity in temporally rich modalities (i.e., EEG, MEG, and ECoG) (Mediano et al., 2023). In fMRI, one challenge is that the Lempel-Ziv algorithm can only be performed with a long time series, but fMRI runs are often short (around 100-500 volumes). Some fMRI analyses therefore compute Lempel-Ziv from concatenated time series from multiple voxels or regions—a process called concatenated LZC (Mediano et al., 2021; Toker et al., 2022; Varley et al., 2020).  

To date, LZC has seldom been used in fMRI. To our knowledge, the only fMRI temporal LZC studies are Catal et al. (2022), Golesorkhi et al. (2022), and Mediano et al. (2021) (as for the other metrics, these neuroscience results are described in the discussion). Nevertheless, LZC is an effective metric; notably, Varley et al. (2020) found that LZC was one of the metrics most correlated with the first principal component extracted from a full sample of complexity metrics, indicating that LZC robustly indexes complexity.

Images (click arrow to open drop-down)

Figure 19. A computation of LZC that is representative of the fMRI literature (Varley et al., 2020; Golesorkhi et al., 2022). Figure created using R Statistical Software (v4.4.3; R Core Team 2021) and based on Golesorkhi et al. (2022). (1) The median of the time series is calculated. (2) The series is binarized, such that timepoints above the median are represented by 1 and timepoints below are represented by 0. (3) This results in a single vector. (4) The Lempel-Ziv algorithm is applied to the vector. This algorithm parses the vector to identify recurring, increasingly long patterns, and adds each identified pattern to a “dictionary.” The length of the dictionary is the classic Lempel-Ziv output. (5) Because this length is dependent on the length of the input time series, the output is normalized (for example, by a factor obtained by running the LZ algorithm on a shuffled version of the input, or by the length of the original sequence). 


8. Principal components analysis

Principal components analysis (PCA) is a dimensionality reduction technique that transforms a large set of variables into a smaller set (Jolliffe & Cadima, 2016). The smaller set captures underlying characteristics of the system that might not be obvious from the raw data. Although PCA and LZC differ in their computation, they are similar in that they both compress data with minimal information loss. 

PCA is an unconventional way of quantifying complexity. While spatial PCA is important in FC analyses (e.g., Damascelli et al., 2022), temporal PCA—which involves computing PCA on voxel-by-voxel time series (Varley et al., 2020)—is rarely used. Importantly, however, temporal PCA is correlated with other measures of temporal complexity, including LZC (strong correlation; Varley et al., 2020) and SE (strong correlation; Varley et al., 2020; Hull & Morton, 2022). PCA has also been used as a measure of brain signal variability, based on the rationale that the number of PCA dimensions reflects the irregularity of the signal (Garrett et al., 2013). PCA exemplifies the diverse and creative ways in which temporal complexity can be described. 
9. Criticality

There are many models of brain complexity (Hancock et al., 2022). One prominent model formalizes the intuitive idea (presented earlier) that complexity involves a balance between order and disorder. In this model, called the “criticality hypothesis,” the brain is considered a dynamical system operating at a phase transition between order and disorder. At this transition—called the critical point—information processing is optimized (Beggs & Plenz, 2003; Marshall et al., 2016; Carhart-Harris et al., 2014; Carhart-Harris, 2018). 

At the critical point, one property of the system that supports optimal information processing is scale-invariance (Hengen & Shew, 2024). When a system is scale-invariant (also called “scale-free”), it is self-similar across all spatial and temporal scales; that is, it has a fractal structure, containing meaningful information whether you zoom in or out. Scale-invariance implies that the system has multiscale memory, with past events influencing the next milliseconds, seconds, and minutes (Hengen & Shew, 2024). By allowing past information to be used in the future, multiscale memory supports optimal information processing (Hengen & Shew, 2024). 

Two seminal approaches for assessing scale-invariance are neuronal avalanches and long-range temporal correlations (LRTCs) (Hengen & Shew, 2024; Beggs & Plenz, 2003; Linkenkaer-Hansen et al., 2001). The following section focuses on neuronal avalanches; the Hurst section describes LRTCs. 
10. Avalanche

What are avalanches? Avalanches are a special form of neural cascade (Beggs, 2022). Cascades are trails of activation spreading from neuron to neuron, where spontaneous activity in one neuron triggers activity in its neighbour. Some cascades are large (involving many voxels over a long time window) and some are small (involving a few voxels and dying off quickly). At the critical point, the distribution of cascade sizes and durations follows a power law distribution, where there are many small and short cascades and few large and long ones (see Fig. 20C-D; Fig. 21). Additionally, there is a power-law relationship between size and duration (see Fig. 20E). Cascades with these power-law properties are called avalanches. 

Some of the earliest discussions of criticality in neuroscience emerged from avalanche data, and in contemporary work, research on the critical point continues to mainly focus on neuronal avalanches (Beggs & Plenz, 2003; Marshall et al., 2016). Avalanches have been studied at multiple scales, from the level of single cells (Bellay et al., 2021), to local field potentials (Beggs & Plenz, 2003), to whole-brain imaging techniques like EEG (Benayoun et al., 2010), MEG (Shriki et al., 2013), and fMRI (e.g., Tagliazucchi et al., 2012; see Table 4). Because of scale-invariance, across all these modalities, avalanche properties can be fit to power laws with relatively consistent exponents (Hengen & Shew, 2024). 
Images (click arrow to open drop-down)


Figure 20. [A] In fMRI, the method used to detect avalanches was introduced by Tagliazucchi et al. (2012). The BOLD time series (either representing a single voxel or the mean of an ROI) is converted to a point process by first normalizing the series by its own SD, then setting a threshold equivalent to a multiple of the SD, and selecting the time points at which the series crosses the threshold from below. The same procedure is repeated for all voxels and ROIs. Panel based on Tagliazucchi et al. (2012). [B] An avalanche is defined as a series of consecutively active time bins, surrounded by empty bins. The size of the avalanche is the total number of activations (across all voxels or ROIs), and the duration is the total number of time bins. Panel based on Xin et al. (2025). [C-E] If the system is near the critical point, the distribution of cluster sizes (P(S)), durations (P(T)), and average size for a given duration〈S〉(T)  can be fitted to power laws with critical exponents α, τ, and γ—where γ is related to α and τ via the relationship shown in panel E, and ideal exponents are α = 1.5 and τ = 2 (figure based on models in Tagliazucchi et al., 2012; also see Beggs & Plenz, 2003; Shew & Plenz, 2013; Petermann et al., 2009; Yang et al., 2023). Note that none of the analyses found in the literature were strictly temporal according to the definition we are using in this review; computations of both size and duration took information from multiple voxels/ROIs into account. 


Figure 21. The temporal properties of avalanches can be further described by plotting their temporal profiles—the activation in each frame across time. Across scales, these profiles display the shape of an inverted parabola. That is, the shape of the temporal profile is the same regardless of the duration of the avalanche, scaled up if the avalanche is long and down if it is short. Note that this “fractal” structure is what allows avalanches to be studied across modalities, from very fine temporal resolution (e.g., local field potential recordings; Beggs & Plenz, 2003) to very coarse resolution (e.g., fMRI; Tagliazucchi et al., 2012). 


11. Hurst

Comparisons of all metrics
We have reviewed over a dozen ways to describe temporal complexity in fMRI. How are these metrics related? And how do they compare to simpler measures, such as variability? A number of studies have sought to answer this question empirically. These studies have found that temporal complexity metrics are correlated with each other, suggesting they capture the same construct. Additionally, they complement measures of variability, suggesting that they are related to fundamental properties of the brain signal. 

Are temporal complexity metrics correlated with each other? 

All temporal complexity metrics are correlated with each other (Varley et al., 2020; Sokunbi et al., 2014). The most highly matched pairs included wPE and RangeEnB (identification accuracy (proportion of correctly identified individuals based on matching two features) of 0.94 in the presence of age, gender, and total-intracranial-volume information; Omidvarnia et al., 2023), SE and LZC (r = 0.86 in dataset 1, r = 0.93 in dataset 2; Varley et al., 2020), and SE and FuzzyEn (r = 0.96; Wehreim et al., 2024). For almost every metric, correlations were positive, meaning that measures increased in tandem. 

The one exception is Hurst. Hurst is negatively correlated with the other metrics; for instance, in Varley et al. (2020), the correlation between SE and Hurst was r = -0.81 in dataset 1 and r = -0.90 in dataset 2. This inverse relationship exists because Hurst is interpreted differently than most complexity metrics. Although Hurst technically ranges from 0 to 1, adult gray-matter values are bounded between 0.5 and 1, with 0.5 representing randomness and 1 representing autocorrelation (e.g., Campbell & Weber, 2022). Thus, in the adult brain, a decrease in Hurst represents an increase in randomness. In contrast, for other measures (entropy-related measures, LZC, and PCA), an increase signifies an increase in randomness. Thus, empirical studies show that Hurst and other measures are negatively correlated. 

Which temporal complexity metric is the best?

We tried to identify a single “best” temporal complexity metric to recommend to our readers. A number of studies have compared metric performance (summarized in Table 2); an ideal metric would have high consistency (for example, high test-retest reliability), discriminate between patient and control populations where relevant, and correlate strongly with other complexity metrics. 

Across studies, there was no clear winner. Metrics that excelled in one study often performed poorly in a different context. For instance, Guan et al. (2022) identified DiffEn as the most reliable metric, but this metric was among the worst in Wang et al. (2013) at the longest TR (2.5s). In the same vein, MSE had unacceptably poor reliability in Wehreim et al. (2024) but in applied contexts (predicting cognitive phenotypes: Omidvarnia et al. (2023); differentiating patients and controls: Guan et al. (2022)) performed fine. The poor agreement across comparison studies is concerning; to ensure their results generalize, we encourage researchers to test metrics with a range of parameters (e.g., various m and r), multiple datasets, and multiple preprocessing pipelines. 

Nevertheless, recent method “extensions” tended to outperform their originals. Top-performing metrics included wPE (Omidvarnia et al., 2023), DiffEn (Guan et al., 2022), PFE (Niu et al., 2020), and FuzzyEn (Wehreim et al., 2024); worst-performing metrics included ApEn (Niu et al., 2020), SE (Niu et al., 2020), MSE (Wehreim et al., 2024), and Hurst (Wang et al., 2013). Note that this is a loose interpretation of sparse/mixed data, and that none of the studies optimized the parameters of the calculations. 

Comparisons of temporal complexity and variability 

Beyond comparing complexity metrics to each other, researchers have examined how they relate to simpler measures like signal variability. Wehreim et al. (2024) showed that correlations between complexity (SE, MSE, FuzzyEn, detrended fluctuation analysis, correlation dimension, effective dimensionality, intrinsic dimensionality) and variance measures (SD, mean average successive difference, MSSD) were mostly low and negative; that is, higher complexity weakly corresponds to lower variability. Pairwise analyses have mixed results. Aligning with Wehreim et al. (2024), Yang et al. (2013) found that MSE and SD were weakly negatively correlated, and Churchill et al., 2016 found that Hurst and variance were moderately negatively correlated; in contrast, Keilholz et al. (2020) found that ApEn and SD were moderately positively correlated. Interestingly, the relationship between complexity and variance could be altered by task performance: The correlation between SD and Hurst is positive (Churchill et al., 2016; He, 2011) but is suppressed by cognitive effort and is lower in older adults than in younger ones (Churchill et al., 2016). In sum, variability and complexity provide complementary information about brain function. (For a summary of the relationship between variability and another common fMRI metric, amplitude of low-frequency fluctuations (ALFF), see Appendix A4.) 
Discussion

The previous sections discussed how to measure complexity. In this section, we discuss what has been found using complexity in fMRI. To foreshadow: BOLD complexity varies across brain regions, is associated with cognitive ability, is affected by task performance, and is altered in many clinical conditions. Notably, despite the fact that complexity metrics are correlated in empirical comparisons (in the previous section), in applied contexts they diverge. In particular, results differ between fine and coarse scales, and entropy and Hurst are sometimes negatively and sometimes positively correlated. We suggest potential explanations and highlight research gaps. There are many open foundational questions in this field that can be answered with simple and inexpensive projects. 

Anatomical distribution of complexity: Complexity is lowest in higher-order regions

Across metrics, complexity varies across brain tissues. Fine-scale entropy is lower in grey than in white matter (SE: Niu et al., 2020; de Carvalho et al., 2025; Wang et al., 2014; ApEn, FuzzyEn, and PE: Liu et al., 2012; Niu et al., 2020; Sokunbi et al., 2015; but see Jiang et al., 2021). Hurst aligns with this pattern, showing higher values in grey than in white matter (Churchill et al., 2016; recall that Hurst and SE are negatively correlated (Sokunbi et al., 2014)). As for specific values, typical SE values in grey matter vary widely depending on the parameters but are typically around 0.5 to 1.5 (Yang et al., 2018), and Hurst values in grey matter are around 0.8 (Campbell & Weber, 2022). Grey matter contains neuronal cell bodies, so low complexity in these regions may reflect synchronized firing and reduced high-entropy neurophysiological noise (Campbell & Weber, 2022). 

Within grey matter, complexity is lower in cortical regions than in subcortical ones (SE: Wang et al., 2014; Nezafati et al., 2020; Wang, 2021; Gale et al., 2021; Hurst: Ciuciu et al., 2014). Within the cortex, complexity tends to be marginally lower in higher-order networks than in lower-order ones (LZC: Golesorkhi et al., 2022). Regions with the lowest SE include the frontoparietal network (Allendorfer et al., 2024; compared to five other networks), frontal and temporal regions (Maximo et al., 2021; compared to parietal and occipital regions), and DN (Wang et al., 2014). Regions with the highest Hurst values (corresponding to low SE) include those in the Dorsal Attention Network and DN (Ciuciu et al., 2014); regions with the lowest Hurst include the motor network (Ciuciu et al., 2014) and limbic network (Campbell et al., 2022). While the terminology varies, together these results suggest a complexity gradient, with lowest complexity in higher-order regions and highest complexity in lower-order and subcortical regions. 

Why is complexity lowest in higher-order regions of the grey matter? One suggestion is that this may be related to the unique properties of grey matter in higher-order regions, such as dense cortical columns (Del Mauro & Wang, 2025) and strong long-range functional connections (Zhen et al., 2024). Another suggestion is that complexity differs in regions involved in internal versus external processing. As described in Campbell & Weber (2022), low Hurst (higher irregularity) may reflect a capacity for external processing, and high Hurst (higher autocorrelation) may reflect a capacity for internal processing. This is because higher irregularity confers agility and preparedness to process unpredictable external stimuli, while higher autocorrelation allows for long-term memory of internal stimuli. So, Hurst is low in regions involved in external processing (motor regions) and high in those involved in internal processing (default mode network).
Fine- versus coarse-scale entropy in fMRI
This section has been changed

Across neuroimaging modalities, the temporal scale of complexity measures spans the very fast (e.g., sub-millisecond electrophysiological fluctuations) to the very slow (e.g., drifts in the hemodynamic response measured with a 1-2s TR). In fMRI, temporal scale can be further coarsened through downsampling. Notably, within the framework of MSE, varying the window parameter m allows for the quantification of SE at either fine or coarse scales—that is, for the comparison of patterns of fine-scale versus coarse-scale fMRI entropy. 

Studies of basic brain anatomy, function, and cognitive function have repeatedly shown that fMRI entropy patterns diverge at fine versus coarse scales. More specifically, statistical contrasts that emerge from high-resolution time series (i.e., a small m value, such as in standard SE computations) “flip” when the analysis is repeated at a lower resolution (i.e., a large m value, such as in the coarser scales of MSE). When anatomy is mapped using fMRI, the contrast between white and grey matter flips with scale (grey < white matter at fine scales, but grey > white matter at coarse scales (Smith et al., 2014)). Similarly, surface area is negatively correlated with SE at fine scales but positively correlated with SE at coarse scales (Zhen et al., 2024), and white matter integrity and cortical myelination are positively correlated with fine-scale SE but negatively correlated with coarse-scale SE (Zhen et al., 2024; McDonough & Siegel, 2018). As for the distribution of entropy across networks, regional differences appear to flip at fine versus coarse scales. While fine-scale SE is lowest in the FPN and DN (Allendorfer et al., 2024; Maximo et al., 2021; Wang et al., 2014), coarse-scale SE is highest in these regions (Trevino et al., 2024; Omidvarnia et al., 2021). The flip appears to occur around scale two (Omidvarnia et al., 2021). Beyond the basics of brain structure and function, the distinction between fine and coarse scales appears to extend to cognition. In a sample of older adults with Alzheimer’s or MCI, Ren et al. (2020) found that the correlation between cognitive performance and SE was positive at fine scales but negative at coarse scales. They reasoned that this was caused by decreased local connectivity along with increased randomness. Ren’s results were consistent with findings from EEG (Sun et al., 2020). That is, correlations and contrasts between entropy and other basic neuroscience metrics change with scale. 

Why does this flip occur? We tentatively suggest that this has to do with the basic qualities of the BOLD signal; as previous authors have suggested, it is possible that fine-scale entropy is related to local information processing and coarse-scale entropy to global processing (Mizuno et al., 2010; Vakorin et al., 2011; McIntosh et al., 2013). This would dovetail with the observation that SE is negatively correlated with the FC at fine scales but positively correlated with FC at coarse scales (McDonough & Nashiro, 2014; Wang et al., 2018; Omidvarnia et al., 2021). For instance, the relationships between cognition and SE observed by Ren et al. (2020) may be due to an association between cognition and strength of local connectivity, wherein a decrease in local connectivity is reflected in both a decreased fine-scale (but not coarse-scale) SE and cognition. 

Importantly, this suggestion is tentative; we acknowledge many limitations. The number of studies thus far is small, and there are limitations inherent in interpreting studies without pre-registered hypotheses. Additionally, it is possible that this is an artifact of the methodological limitations of MSE. One possible explanation is methodological limitations. Coarse-scaling smoothes out irregularities, and work in EEG suggests that MSE may be biased downward at coarse scales (see Appendix A2). There is little basic-methods work that directly addresses the relationship between fine- and coarse- scale entropy in fMRI. Thus far, empirical comparisons of SE and MSE have used mean MSE (Omidvarnia et al., 2022; Omidvarnia et al., 2023; Wehreim et al., 2024; summarized in the “Comparisons of All Metrics” section) 
Images (click arrow to open drop-down)

Figure 28. SE is lowest in grey matter (GM) compared to cerebrospinal fluid (CSF) and white matter (WM). Original image; brain from MNI and relative GM/CSF/WM values stylized based on results from Niu et al. (2020).


Complexity and trait cognitive ability: High entropy is associated with higher cognitive ability

Cognitive ability refers to how efficiently an individual can carry out mental processes such as executive function, attention, memory, language, and problem-solving (Breit et al., 2024). It can be quantified in several ways, including using assessments of intelligence (Saxe et al., 2018) and with clinical-condition-specific assessments, notably measures of mild cognitive impairment (MCI) in Alzheimer’s (Zhang et al., 2025). In this section, we discuss intelligence in healthy individuals, cognitive ability in Alzheimer’s, and sedation. Although we recognize that unique neurobiological factors are at play in each of these groups, we present them together to highlight similarities. In particular, we highlight that complexity is a general feature of brain function and is consistently associated with cognitive ability. There is strong support for a complexity/cognitive ability gradient, with the highest cognitive ability (high-intelligence healthy individuals) having the highest complexity, followed by decreases in complexity with increasing impairment, and sedation at the bottom. 

Intelligence

A number of studies have described a relationship between trait intelligence and brain dynamics at rest. In theory, resting-state dynamics represent the brain’s readiness to process environmental inputs (Saxe et al., 2018); as such, individuals with higher intelligence should have resting-state dynamics that lie closer to the critical point, where information processing and dynamic range are optimized (Xu et al., 2021). In line with theoretical predictions, entropy-related metrics can predict intelligence (Omidvarnia et al., 2023; Liu et al., 2020; Yang et al., 2022), and individuals with higher intelligence have higher entropy (Saxe et al., 2018; Omidvarnia et al., 2021; Sokunbi et al., 2011; Yang et al., 2013 (cognitive ability); but see Wijesinghe et al., 2025; Wang, 2021), potentially reflecting enhanced ability to process unpredictable information (Saxe et al., 2018). As well, individuals with higher intelligence have avalanche dynamics that fall closer to the critical point (Xu et al., 2021; Xin et al., 2025). Intelligence has a quadratic relationship with Hurst that peaks at H = 0.89; that is, intelligent brains are near criticality, but sub-critical (Xin et al., 2025). 

Which regions are important? The relationship between entropy and cognitive ability varies across regions (Del Mauro & Wang, 2024). The relationship between entropy and intelligence was strongest in the prefrontal cortex, inferior temporal lobes, and cerebellum (Saxe et al., 2018); regions whose critical dynamics were positively correlated with intelligence were the prefrontal cortex and inferior parietal cortex (Xu et al., 2021). These are the same regions that are associated with intelligence in the broader literature (Yoon et al., 2017). There has been some suggestion that SE is particularly well-correlated with surface area in regions involved in intelligence (Del Mauro & Wang, 2025). 

Mild cognitive impairment and Alzheimer’s

As individuals progress along the Alzheimer’s continuum—from healthy (HC), to mild cognitive impairment (MCI), to clinical Alzheimer’s (AD)—both their cognitive performance and entropy decreases (HC > MCI: de Carvalho et al., 2025; Pena et al., 2022; Jann et al., 2023; Zheng et al., 2020; Smith et al., 2015; HC > MCI > AD: Niu et al., 2018; Jann et al., 2023; Wang et al., 2017c; HC > AD: Grieder et al., 2018; but see Xue al., 2018; but the relationship may differ across scales; Ren et al., 2020). Hurst is increased in Alzheimer’s (Maxim et al., 2005; Warsi et al., 2012). These temporal/fMRI results are consistent with the broader complexity literature (Sun et al., 2020). Why does complexity decrease along the Alzheimer’s continuum? The same factors that apply to healthy individuals (proximity to criticality) may also apply here. In Alzheimer’s specifically, decreased complexity may originate from the degradation of neural connections, which is caused by the buildup of plaque and neuronal cell death (Sun et al., 2020; the disconnection syndrome hypothesis of Alzheimer’s: Delbeuck et al., 2003). Complexity may be closely related to tau-PET; Jann et al. (2025) found that classifier (HC vs AD) performance was similar for SE, mean MSE, and tau-PET.

Consciousness

Cognitive performance is at a floor when individuals are sedated. We found only one study that assessed the change of temporal complexity with sedation: Varley et al. (2020). This found that all measures of complexity decreased with increasing sedation. That is, SE and LZC decreased, while Hurst, which is negatively correlated to these metrics, increased. This was consistent with the broader complexity literature. Although studies on consciousness and temporal complexity are scarce, there is a rich literature on the relationship between consciousness and spatiotemporal complexity (Sarasso et al., 2021). Complexity is important in many theories of consciousness because conscious experience is thought to involve both integration—the ability to combine information from varying sources, such as auditory and visual information, into a coherent experience—and segregation—the ability to distinguish between experiences (Sarasso et al., 2021). During sedation, it’s possible that neural activity becomes uniform (i.e., loses complexity), which disrupts both integration and segregation, leading to loss of consciousness (Sarasso et al., 2021). 
Complexity and task engagement: Entropy and Hurst results diverge

Early studies of complexity focused on resting-state data, but in the past five years investigations have incorporated task paradigms as well. How is neural complexity affected by the performance of a task? In this section we describe how entropy and Hurst are both affected in consistent ways, but conventional interpretations break down when they are considered together. A decrease in entropy is conventionally interpreted as an increase in predictability, and an increase in Hurst is likewise interpreted as increase in predictability. But this interpretation fails when applied to task-based results. 

Entropy

Entropy (and a related measure, LZC) decrease in task compared to rest (Çatal et al., 2022; Nezafati et al., 2020; Golesorkhi et al., 2022; Lu et al., 2024; but see Mediano et al., 2021). This was consistent across a variety of paradigms (e.g., working memory, social processing, task-switching, memory encoding). In terms of regional differences, the decrease was larger in higher-order networks than in lower-order ones. For some studies, this manifested as a larger decrease in higher-order than lower-order networks (Golesorkhi et al., 2022). For others, this manifested as a decrease in higher-order regions and an increase in lower-order ones (Camargo et al., 2024 (decrease in regions involved in higher-level cognitive processes and increase in sensorimotor and perception networks)). 

Why does entropy decrease in higher-order networks? Possibly, task-activated regions have lower entropy. Consistent with this hypothesis, Gale et al. (2021) found that, across a variety of tasks, regions that were active (i.e., that were predicted to be active based on the literature) had lower complexity than regions predicted to be less relevant. Additionally, they found a strong negative correlation between BOLD amplitude and entropy in task scans (but not in resting-state scans). This suggested that regions recruited for a task had signals that were more predictable than those in other regions. They did not find this correlation in resting-state scans, suggesting that a decrease in irregularity might be a unique feature of tasks. 

However, there are reasons to be skeptical of the above explanation. Entropy is sensitive to aspects of the BOLD signal that are not directly tied to activity levels; notably, it changes in regions that are unchanged or deactivated during tasks (e.g., the DN) (Nezafati et al., 2020). Additionally, there is some suggestion that entropy differences across task-activated versus task-deactivated regions exist at rest (Lin et al., 2022), which implies that entropy reflects more than simple task-related activation. 

Hurst

Because Hurst is inversely related to entropy, we would expect it to increase during tasks. Instead, it decreases during tasks, reflecting decreased long-range memory and greater irregularity (He, 2011; Akhrif et al., 2018; Omidvarnia et al., 2022). Dovetailing with this, Hurst decreased more with increasing task demands (novelty and difficulty: Churchill et al., 2016; effort: Barnes et al., 2009; load: Omidvarnia et al., 2021) and performance slowness (Wink et al., 2006; 2008; Suckling et al., 2018). These fMRI results are consistent with the rest of the Hurst literature (as summarized in Campbell & Weber, 2022). 

Why does Hurst decrease? It’s possible that external stimulation draws the brain away from default-mode processing, a shift which involves the decoupling of neuronal groups; decoupling leads to high-frequency domination (or the suppression of low-frequency activity) which decreases the correlation of scale-free signal (Campbell & Weber, 2022). 

Interpretation

It is clear that temporal complexity is affected by task. Task performance affects entropy, LZC, and Hurst; in our literature search, we only found one fMRI paper that showed no change in temporal complexity in task compared to rest (Leavitt 2022), and that null finding could be explained by a low sample size and methodological issues. However, what exactly is happening during task performance is unclear. Our results highlight a divergence between entropy (and entropy-related measures such as LZC) and Hurst. Despite solid evidence that entropy and Hurst are inversely correlated (Varley et al., 2020), both increase during task. This is important because this divergence is not clear in other contexts, such as direct empirical comparisons (see the “Comparisons of All Metrics” section) or studies of cognitive performance. 

What are the possible explanations for this discrepancy? A first possibility is that this results from a subtlety of the hemodynamic response that is not fully captured by either metric. A second possible explanation is that Hurst and SE describe complexity at different scales: Hurst describes long-range correlations, and SE describes short-term irregularity. It is possible that, during task performance, both long-range correlations and short-term irregularity decrease, which results in the observed decreases in both metrics. Another possibility is that there is an increase in short-term irregularity, but it is not detectable in fMRI (in EEG, entropy increases (rather than decreases, as in fMRI) during task (Pakniyat et al., 2024; Amin et al., 2013; Grundy et al., 2019 (increase with task demands))). In any case, the idea that the balance of fine/coarse scale irregularity would shift with task performance is consistent with the broader literature on task-related FC changes (recall that fine-scale complexity is related to local FC, while coarse-scale complexity is related to global FC). We tentatively draw a comparison to a phenomenon from the anaesthesia literature termed paradoxical excitation, wherein rapid delivery of a sedative leads to an unexpected increase neural complexity (Maschke et al., 2022); both these phenomena feature an external perturbation that, presumably, shifts the dynamic regime of the system such that resting-state rules no longer apply. Concretely, this idea could be confirmed by computing both SE and Hurst on both resting-state and task data, potentially from the Human Connectome Project. 
Movie-watching: Complexity patterns distinct from tasks

Complexity patterns may differ in naturalistic paradigms compared to task paradigms. Campbell et al. (2022) tentatively suggested that Hurst may be highest in movie-watching, lower in rest, and lowest in task. The patterns of Hurst they observed during a movie-watching paradigm differed from the task-related patterns described in the literature. During movie-watching, compared to at rest, whole-brain Hurst was increased, particularly in the visual, somatomotor and dorsal attention networks—possibly because fractality supports the processing of continuous, naturalistic stimuli (Campbell et al., 2022). Similarly, Mediano et al. (2021) found that LZC is highest in movie-watching, lowest in rest, and medium in task. Song & Wang (2024)c observed lower SE in sensory cortex and higher SE in association cortex.  
Clinical findings

Historically, complex-systems approaches to physiology have modelled disease and dysfunction as abnormalities in the dynamic processes of the system (Lau et al., 2022). Beginning with the seminal work by Lipsitz & Goldberger (1992) showing that aging involves a loss of complexity in peripheral electrophysiological signals, physiological health and complexity have been thought to increase in tandem. This historical view suggests that, in the brain, optimum function consistently correlates with increased complexity. However, results in the literature are heterogeneous. While it is clear that dysfunction involves altered complexity, alterations occur in both directions, and the link to pathology is unclear. 

This section discusses clinical conditions that have been relatively well-studied (>5 studies) using temporal complexity metrics in fMRI. We attempt to connect findings to those from other neuroimaging modalities and highlight open questions and how they can be answered. 

Attention-deficit hyperactivity disorder

Attention-deficit hyperactivity disorder (ADHD) is a disorder that involves hyperactivity and inattention (Wang et al., 2017). We found consistently decreased entropy in adults and pre-adolescents (Zhang et al., 2025; Sokunbi et al., 2013; Zhang et al., 2024; Guan et al., 2023). This decrease was particularly evident in the regions involved in cognitive control (frontoparietal network (Zhang et al., 2024); executive function network (Zhang et al., 2025); frontal and occipital regions (Sokunbi et al., 2013)), which is consistent with the fact that regions involved in cognitive control are well-established to be altered in ADHD (Zhang et al., 2024). Decreased entropy in adult ADHD could reflect that the brain is operating in a more rigid, less flexible pattern, resulting in a reduced ability to adapt to changing demands. Our fMRI findings were consistent with the EEG and MEG literature (as reviewed in Zhang et al., 2024 and Hernández et al., 2023). Additionally, clinical symptoms of ADHD (impulsivity and inattention) can be predicted using entropy (Wang et al., 2017). 

Autism spectrum disorder

Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by social impairments, cognitive dysfunction, atypical motor and sensory functioning, and repetitive stereotyped behaviours (Lord et al., 2020). The neuropathology of ASD involves abnormal neural connectivity, both structural and functional (Ecker et al., 2017). In particular, ASD involves local hyperconnectivity and sparser long-range connections (Ecker et al., 2017). Consistent with the idea that fine-resolution temporal complexity corresponds to local information processing (Dreszer et al., 2020), studies using EEG—a fine-grained measure—consistently show a decrease in complexity in ASD (Hernández et al., 2023). That is, reduced local connectivity is associated with reduced fine-scale entropy. 

As expected, in our review of fMRI studies, we found alterations in complexity in ASD; for instance, the trajectory of changes in Hurst during childhood is altered in ASD (Linke et al., 2024). However, as for whether ASD involves increases or decreases in complexity compared to controls, results were heterogeneous. We found mixed directions for entropy alterations in ASD (increase: Fu et al., 2024; decrease: Zhang et al., 2020; mixed: Maximo et al., 2021; Xiao & Jones, 2025; no diff: Easson & McIntosh, 2019; Chi et al., 2025). We also found mixed Hurst results (decrease: Lai et al., 2010; Uscǎtescu et al., 2023; increase: Dona et al., 2017). This discrepancy may exist because fMRI has a moderate temporal resolution; that is, EEG and fine-TR fMRI studies report reduced entropy, while fMRI studies with moderate TRs report more increases. This theory is tentative. 

Schizophrenia 

Schizophrenia is a neuropsychiatric disorder with symptoms including hallucinations, delusions, disorganized behaviour, and avolition (McCutcheon et al., 2020). In our review of fMRI studies, we found heterogeneous entropy effects across studies (decrease: Liu et al., 2024; Hager et al., 2017; Yang et al., 2015; increase: Sokunbi et al., 2014; Li et al., 2025; mixed: Zhang et al., 2021; Wang et al., 2017b; Xue et al., 2019; Guan et al., 2023; no difference: Guan et al., 2022). Hurst decreased in the two papers we reviewed (Sokunbi et al., 2014; Uscătescu et al., 2023). 

Hernández et al., 2023 also observed this heterogeneity in their broader review. They proposed that it might mirror the heterogeneity of schizophrenia populations and paradigms used to study complexity. Schizophrenia involves both positive symptoms (e.g., hallucinations and delusions) and negative ones (depression and avolition) at different times; there is evidence from EEG that these are associated with different patterns of complexity alterations (Raghavendra et al., 2009). Additionally, studies of complexity use different ages, medication status, and tasks—all of which are associated with different complexity patterns (Fernandez et al., 2013; Takahashi et al., 2010; Kirsch et al., 2000). For instance, Huang et al. (2025) found that Shannon entropy was significantly lower in patients with treatment-resistant schizophrenia than in healthy controls, but did not differ between patients with non-treatment-resistant schizophrenia versus controls. Additionally, alterations in entropy may be regionally heterogeneous and related to other dimensions of neural function; for instance, Bassett et al. (2012) found that whole-brain Shannon wavelet entropy did not differ between patients and healthy controls; however, in both groups, wavelet entropy differed across brain regions, with regions with higher wavelet entropy having stronger functional connections than regions with low wavelet entropy. The problem with this explanation is that it implies that studies with restricted patient subpopulations should report similar results, but in our small sample of studies, we did not find evidence for this. 

Depression

Depression is a disorder of persistent low mood (Bains & Abdijadid, 2025). Recently, Xin et al. (2022) showed that depressed brains were in a sub-critical state, meaning that they were less excitable. Specifically, they observed that their depressed sample had smaller avalanche sizes and lower branching ratios (i.e., activation is dampened quickly). This was accompanied by weaker functional connections (both local and global). Hernández et al. (2023) observed that EEG and MEG studies of depression consistently report decreased entropy. Together, these results suggest that depression involves a state of neural disconnection, where neural activity is reduced, signals fail to propagate, and communication across regions is reduced. 

However, in our review of fMRI entropy studies, we found mixed results. We found both entropy increases and decreases, and regions with differences were inconsistent across studies (increase: Song et al., 2024; Ho et al., 2017; decrease: Xue et al., 2019; mixed: Liu et al., 2020; Lin et al., 2019). Tentatively, depression involves whole-brain decreases in complexity along with increased complexity in the frontal regions, potentially due to disruptions in the control of cortical inhibition, at moderately coarse scales (Lin et al., 2019; Ho et al., 2017); that is, previous studies accurately described a whole-brain decrease in complexity, but, due to their low spatial resolution and fine temporal resolution, failed to detect an increase in frontal regions. However, this is conjecture. Overall, there have been few studies on complexity in depression, and more research is needed. 

Bipolar disorder

Bipolar disorder is characterized by shifts between manic, depressive, and mixed mood states (Emilien et al., 2007). Bipolar has heterogeneous entropy patterns (increase: Liu et al., 2023; Li et al., 2025; decrease: Hager et al., 2017; mixed: Guan et al., 2023; Xiang et al., 2021; Zhang et al., 2021). Entropy alterations in bipolar span the whole brain, including regions in the sensorimotor network, DN, central executive network, FPN, sensory networks, and limbic and memory-related regions (Guan et al., 2023; Liu et al., 2023a; Li et al., 2025; Zhang et al., 2021; Xiang et al., 2021). Similarly, multifractal activity is altered in the sensorimotor network, default mode network, and visual network (Guan et al., 2024). 

What could account for this breadth of alterations? Firstly, we note that this diversity is consistent with the broader bipolar neuroimaging literature, which is marred by mixed findings and has failed to coalesce into a single theoretical paradigm (Maletic & Raison, 2014). Like with schizophrenia, bipolar involves clinical heterogeneity, and many studies fail to account for mood or medication state (both of which have a demonstrated impact; Maletic & Raison, 2014).

Nevertheless, among the breadth of potential frameworks for bipolar, all emphasize that no single region accounts for the deficits, but rather it is the functional connections between regions that give rise to symptoms (Maletic & Raison, 2014; Wu et al., 2024); for instance, a classic framework suggests that bipolar could be explained by abnormalities in a prefrontal-striatal-pallidum-thalamic-limbic circuit (Wu et al., 2024). This accounts for the breadth of regions with altered complexity. Reassuringly, the regions with altered complexity are consistent with those described by other neuroimaging studies; in the literature, the most frequently reported regions are the DN, FPN, sensorimotor, and salience networks (Wu et al., 2024). 

We note that bipolar and schizophrenia traits overlap (Li et al., 2024), and many complexity studies investigate both disorders (again, with mixed findings; Li et al., 2024; Guan et al., 2023; Zhang et al., 2021; Hager et al., 2017; Ji et al. (2025); Li et al., 2025). 

Aging

Using a lifespan sample (ages 6-85), Wijesinghe et al. (2025) showed that mean MSE rises and falls in an inverted-U-shaped curve. That is, it increases through childhood and adolescence, peaks at age 23, then slowly decreases. Regions mature at different rates, with posterior ones (e.g., occipital and temporal areas) peaking earlier than anterior ones (e.g., frontal and parietal lobe). Using the same participants, Niu et al. (2022) showed that global PE peaks at age 40, with the primary sensory network peaking first, then the control network, followed by the DN—results consistent with Wijesinghe et al. (2025). 

Potentially capturing the right end of the entropy-lifespan curve, Yang et al. (2018) described a steady decrease in mean MSE from ages 21-89, Chang et al. (2024) described a decrease in SE from 58-77, Sokunbi et al. (2015) described a decrease in FuzzyEn (but not SE) from 19-85, Liu et al. (2012) found lower ApEn in older adults (66±3) than younger ones (23±2), Sokunbi (2014) found lower SE in older adults (59±10) than younger ones (29±9), and Kielar et al. (2016) found lower SE in older adults (66±2) than younger ones (25±1). Decreases in entropy over the lifespan may be associated with changes in dendritic density and synapse number (Wijesinghe et al., 2025). 

However, there are also reports of increases in entropy across the lifespan. Del Mauro et al. (2025) found a consistent increase in SE across the lifespan (ages 8-89); that is, unlike other lifespan projects, this study did not find a U-shaped age-SE curve. This discrepancy might be explained by a divergence between healthy controls and individuals on the Alzheimer’s continuum: Wang & The Alzheimer’s Disease Neuroimaging Initiative (2020) found that in older adulthood (ages 56-95), SE decreased with age in Alzheimer’s but increased in healthy aging. They presented a framework wherein entropic “pressure” increases with age but (in healthy aging) is counteracted by an increased functional reserve, resulting in this gradual increase in entropy. This was consistent with Zhang et al. (2025)a. 

Assuming that lower Hurst corresponds to higher entropy, Hurst results mostly align with entropy results. Both Wink et al. (2006) (young adult group ages 20-25, older adults aged 60-70) and Churchill et al. (2016) (20-33, 61-82) describe an increase in Hurst with age. Dong et al. (2018) (ages 19-85) described an increase in Hurst in the frontal and parietal lobe along with a decrease in the insula, limbic, and temporal lobe. Why did Dong’s group not find an inverted-U, as seen with lifespan projects that use entropy? It is possible that this discrepancy reflects either a true difference in Hurst, an age threshold effect (Dong’s youngest participant was 19, as opposed to 6), or a small sample size (N=116, as opposed to 504); there is a research gap here. 

Infants

The small number of studies in neonates thus far suggest that whole-brain Hurst may increase and SE may decrease over the course of early infancy, with regional differences reflecting known developmental trajectories (Drayne et al., 2024; Mella et al., 2024; Zhao et al., 2024). In preterm-to-term-age infants, Hurst increases with age, with the steepest rate of increase in the motor and sensory regions, potentially reflecting the earlier maturation of these networks (Drayne et al., 2024; Mella et al., 2024). In neonates, SE positively correlates with postnatal age in the sensorimotor-auditory cortex and negatively correlates with gestational age in the association cortex, potentially reflecting a previously described “sensorimotor-association” developmental pattern (Zhao et al., 2024). 

Epilepsy

Epilepsy is thought to be caused by an imbalance between excitatory and inhibitory (E/I) signaling, wherein a shift towards excitation causes neural overactivity, resulting in spontaneous seizures (Xie et al., 2024). Hurst can be used as a measure of the E/I ratio, with reduced Hurst signifying a shift toward excitation (Xie et al., 2024); consistent with theory, Hurst is reduced in epilepsy (Xie et al., 2024). As for entropy, consistent with the idea that higher entropy signifies lower Hurst, the two existing fMRI studies show decreases in entropy in epilepsy. Gupta et al. (2017) found that Rolandic epilepsy patients displayed whole-brain increases in wavelet entropy; Zhou et al. (2019) found that right temporal lobe epilepsy patients had increased SE in the main affected region (right temporal lobe). 

Nicotine

SE is elevated in chronic smokers (Li et al., 2016; Velioglu et al., 2024; Jordan et al., 2023), particularly in networks associated with cognition (Li et al., 2016; Velioglu et al., 2024). This is consistent with previous indications that smoking increases activity in these networks, and with evidence that smoking enhances cognition and attention (Li et al., 2016; Velioglu et al., 2024).

Other studies
fMRI temporal complexity has also been studied in relation to divergent thinking (Shi et al., 2020), cognitive performance in children (Amalric & Cantlon, 2023), chronic fatigue syndrome (Shan et al., 2018), classical trigeminal neuralgia (Liu et al., 2023), diabetes (Yuan et al., 2022), extroversion (Lei et al., 2013), glioma (Festor, 2022), insomnia (Zhou et al., 2016), motor sequence learning (Jager et al., 2023), moyamoya disease (Lei et al., 2021), multiple sclerosis (Zhou et al., 2016a), obsessive-compulsive disorder (Zhang et al., 2025; Jiang et al., 2021), pain (Del Mauro et al., 2024; Del Mauro et al. (2025)a), Parkinson’s (Su et al., 2022), progressive supranuclear palsy (Whiteside et al., 2021; Kadota et al., 2021), sleep (Kung et al., 2022), neurotransmitters and structure-function coupling (Song & Wang (2024)b), transcranial magnetic stimulation and intermittent theta burst stimulation (Song et al., 2019a; 2024; Liu et al., 2025), substances (alcohol: Sevel et al., 2020; Weber et al., 2014; caffeine: Chang et al., 2018; cocaine: Wang et al., 2017a; marijuana: Jiang et al., 2023; intranasal oxytocin: Song & Wang, 2024; psilocybin: McCulloch et al., 2024), cerebral blood flow (Song et al., 2019), occupation (Wang et al., 2018a), estrogen (Zhao et al., 2024), progesterone (Song & Wang, 2024a), worry (Churchill et al., 2015), reappraisal (Gao et al., 2018), neuroticism (Gentili et al., 2017), rumination (Gao et al., 2023), social anxiety (Gentili et al., 2015), fear (Tetereva et al., 2020), anxiety (Fan et al., 2023), memory encoding (McDonough et al., 2019), walking (Zhou et al., 2018), motion during scan acquisition (de Vries et al., 2019), resting-state microstates (Erbil & Deshpande, 2024), traumatic brain injury (Sharma et al., 2024; Allendorfer et al., 2024; Dona et al., 2017), stroke (Kielar et al., 2016; Liang et al., 2020; Gutiérrez Pereira et al., 2022), and noise pollution (Stobbe et al., 2024). 

These studies are described in more detail in Tables 1-4.
 
Conclusion
Will do last. 


Appendix
A1. How to choose m and r for SE
Neuroscientists seeking to use SE immediately face the problem of choosing parameter values. As summarized in Table 1, a variety of m- and r-pairs have been reported. Broadly speaking, using a high m-value along with a low r-value (a “stringent criterion”) results in fewer matches and thus increased variation in SE, whereas a relaxed criterion (low m-value and high r-value) results in more matches and a reduced ability to discriminate between conditions (Lake et al., 2002; Fig. A1). There are a range of ways to choose optimum parameters for SE, ranging from the computationally exhausting to the very simple. (1) The first of these methods was introduced using heart rate data and involved first determining a range of m using an autoregressive model, then choosing r by computing SE on 200 randomly selected datasets (Lake et al., 2002). (2) In fMRI, Yang et al. (2018) selected parameters that would minimize relative error in regions with minimal physiologic information (i.e., cerebrospinal fluid); they identified a surprisingly large range of suitable pairs, including m = 1 and r ≥ 0.2, m = 2 and r ≥ 0.35, m = 3 and r ≥ 0.6, and m = 4 and r ≥ 0.75. Curiously, however, Easson & McIntosh (2019) applied this technique and obtained slightly different results: The combinations they identified as acceptable were m = 1 and r ≥ 0, and m = 2 and r ≥ 0.2. (3) More common in practice, researchers may choose the best parameters for their particular dataset/contrast using the receiver operating characteristic curve, which quantifies the discriminatory ability of a statistic (e.g., Sokunbi et al., 2013; Sokunbi, 2014; Fu et al., 2021). (4) Researchers may also perform calculations using multiple parameter values to verify that changing the parameters does not change the result (e.g., Festor 2022; Grandy et al., 2016 (simulated data)). (5) Roediger et al. (2024) developed a tool for selecting m and r based on autoregressive models and a grid search function. (6) Finally, there is the simplest option: to simply apply previously used parameters. This appears to be the most common selection method in practice, and the most popular choices are m = 3 and r = 0.6 (see Table 1). In summary, SE appears to be fairly robust to parameter selection, and a range of values is appropriate. 


Figure A1. Top panel: A “stringent” criterion (high m, low r) results in few matches and thus a low SE. Bottom panel: A “relaxed” criterion (low m, high r) results in many matches and thus a high SE. Original image. 
A2. MSE in grey matter; limitations of MSE

Grey matter may have a signature MSE curve. McDonough et al. (2014) suggested that, across imaging modalities (EEG, MEG, and fMRI), MSE for grey matter approximates a skewed inverted-U across scales, with low values at fine resolutions, high values at medium resolutions, and low values at coarse resolutions (Fig. 20; example in MEG: Kielar et al., 2016). The left end of the inverse-U may be captured only by EEG/MEG or fMRI with sufficiently low repetition time (TR); that is, low-TR fMRI studies should describe an inverse-U-shaped MSE curve, while high-TR fMRI studies should describe a decreasing slope (McDonough et al., 2014). However, comparing findings across studies is challenging. Results of fMRI studies that report MSE across scales are summarized in Figure A2. Consistent with McDonough et al. (2014)’s prediction, most papers were classified as having either an inverse-U-shaped or negative-slope MSE curve. The shape of the curve was loosely related to TR; within the inverse-U group, the four studies with the longest TR all peaked at scale 2 (i.e., they had a very short left end of the curve). Shape was not explained by a systematic difference in preprocessing between these two groups (for instance, it is not the case that reports “missing” fine-scale low values were all low-pass filtered). Two studies reported whole-brain, healthy-control MSE curves that were neither inverse-U-shaped nor negatively sloped—these were instead flat (Niu et al. (2018), TR = 3, m = 2, r = 0.35, grey matter) or mildly positively sloped (Zhang et al. (2021), TR = 2, m = 2, r = 0.3, grey matter); the reason is unclear. 

Unfortunately, it is unclear whether the aforementioned decrease in entropy at coarse scales is a result of methodological issues. Work in EEG suggests that MSE may be biased downward at coarse scales (Kosciessa et al., 2020). Specifically, MSE is lower with a narrower similarity bound (i.e., r*SD), and coarse-graining decreases the variance of the new series; so, when a global similarity bound (i.e., r*(SD of the entire series)) is used, MSE is systematically underestimated at coarse scales (Kosciessa et al., 2020). Despite recommendations against it, 90% of MSE-EEG studies use global similarity bounds (Kosciessa et al., 2020). Kosciessa et al. (2020) and Lu & Wang (2021) offer methods for correcting bias. Given that so many fMRI-MSE studies describe a downward slope, experimentally exploring coarse-scale MSE bias in fMRI within neuropsychiatric populations is an important future direction. 




Figure A2. An attempt to map the past decade of fMRI-MSE results onto a stylization of the inverted-U curve proposed by McDonough et al. (2014). For each study, TR, m, and r are listed. The x-axis represents scale, and across studies the leftmost value is always 1. MSE is plotted on the y-axis. The curve is a hypothetical representation that does not necessarily correspond to actual data. Studies were classified as either having an inverse-U-shaped or negative-slope MSE curve. Most of these studies did not test for the significance of inter-scale comparisons, so classification was performed by visual inspection of plots. Only studies that explicitly reported MSE across scales were inspected (which excludes the vast majority of fMRI MSE papers); additionally, results that did not fall into either category (i.e., that had either a flat or a positive slope) were excluded from the figure and are instead described in-text.
A3. How to choose m and r for MSE 
As with SE, users must select appropriate m- and r-values and consider the length of the time series. The process of parameter selection is similar to that of SE. Yang et al. (2018) explored parameters suitable for MSE and concluded that m = 1, r = 0.2-0.45 were reliable for most scale factors (Fig. A3). Beyond this rule of thumb, the strategies used in the MSE fMRI papers we reviewed were as follows: Running analyses with different parameter pairs and choosing those that result in the greatest number of significant comparisons (Yang et al., 2013; Niu et al., 2018); running a robustness analysis to verify that changing the parameters does not change the results (Smith et al., 2014; Zhen et al., 2024); choosing the pairs with highest discriminability using the receiver operating characteristic curve (Yang et al., 2022); simulating noises with different parameters and selecting those that result in stable SE values across scales (Lin et al., 2019); and simply using parameters from prior MSE studies (the remaining papers). 



Figure A3. MSE changes with changes in parameters m and r.  Figure stylized based on results from Yang et al. (2018).

A3. Comparisons of temporal complexity and amplitude of low-frequency fluctuations

Over the past 30 years, there has been extensive interest in low-frequency fluctuations in fMRI (beginning with the observation that at rest these have physiologically meaningful correlations; i.e., resting-state networks; Biswal et al., 1995). Zang et al. (2007) introduced the amplitude of low-frequency fluctuation (ALFF) by calculating the square root of the power in a particular frequency range. To reduce the sensitivity of ALFF to noise, Zou et al. (2008) introduced fractional ALFF (fALFF), which scales the power in a range by that of the whole frequency range. 

There is no statistical relationship between SE (a statistical measure) and fALFF (the relative power of low-frequency signal). However, in theory, low-frequency signal should contribute less to the SE measurement than high-frequency signal, and brain areas with greater low-frequency power should have lower SampEn (Song et al., 2019). Consistent with this prediction, Song et al. (2019) found no correlation between the two metrics in most brain regions, but strong negative correlations in areas with elevated low-frequency fluctuations during rest (visual cortex, anterior inferior temporal cortex, striatum, motor network, precuneus, and lateral parietal cortex). They also found moderate to high correlations in areas suggested to have high information processing (orbito-frontal cortex and posterior inferior temporal cortex). Zhang et al. (2021) computed SE and fALFF across the whole brain and found that whole-brain results were highly correlated. In contrast, Liang et al. (2020) found that the correlation between SE and fALFF was mostly non-significant.
